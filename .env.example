# Ollama Configuration
# Make sure Ollama is running locally (ollama serve)
# Install the model: ollama pull llama3.1
OLLAMA_BASE_URL=http://localhost:11434

# Pinecone Configuration
# Get your API key from: https://app.pinecone.io/
# Pinecone client reads from PINECONE_API_KEY environment variable or ~/.pinecone/config
PINECONE_API_KEY=your_pinecone_api_key_here
PINECONE_INDEX_NAME=your_index_name_here
PINECONE_NAMESPACE=your_namespace_here

# Tavily Search API (Optional - for web search agent)
# Get your API key from: https://tavily.com/
TAVILY_API_KEY=your_tavily_api_key_here
